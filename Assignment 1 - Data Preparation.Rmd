---
title: "Assignment I â€“ Data Preparation"
author: "Mohamed Megahed"
date: "3/4/2020"
output: html_document
---
### A Comprehesive Data Exploration and Analysis on CSV and JSON files:

The Open Data 500 is the first comprehensive study of U.S. companies that use open government data to
generate new business and develop new products and services. Open Data is free, public data that can be
used to launch commercial and nonprofit ventures, conduct research, make data-driven decisions, and solve
complex problems

We will analyze two datasets 
- US_agency.csv and 
- US_companies.csv. 

```{r, include=FALSE,message = FALSE}
library(dplyr)
library(tidyverse)
library(jsonlite)
```

Loading the dataset :
```{r}
US_agency <- read_csv("us_agencies.csv")
str(US_agency)

US_companies<- read_csv("us_companies.csv")
str(US_companies)
```
*The load time in reading the dataset using read_csv of tidyverse package is faster compared to read.csv. The main advantage of this function is it parses the file while loading to environment.*

Tasks to explore the dataset and answering the following questions:

**1. Are there any missing columns?**

On a general exploration through data, there is no missing columns. However if the datasets are related, their should be a primary key. There is no primary/unique column in us_agencies dataset which concludes a missing column.

**2. Are there any missing column names or errors in the column names? If so, name those columns.**
```{r}
colnames(US_agency)
colnames(US_companies)
```
Yes, we have errors in some column names: 
for the dataset 'US_agency'

1. "used_by_category" the name of this column should be "category of agency work" or " agency work category" as the agency is not used by category  

2. "url" is not clear ftom its name whether it is the agency url or the subagency 

3. "used_by"i t is not clear which one is used by the adency or the subagency

4. "used_by_fte" it is not clear from the name what fte represent or what this range is if we assume it is the number of    employees in each subagency the name sholud change also cause used_by is inappropriate term here


for the dataset 'US_companies'

1. "company_name_id" column it just repeats the campany name, so we can use company name column as an id or there should be a numeric id for each company

2. ""data_types"" the names is stating the types of data but the data stored in this variable is about the data ncategory it has so I guess this column is not clear by its name. 


**3. Are there any values in the columns missing?**
### First: **US_agency** 

Let's calculate the number and percent of missing data and plot them
** checking if there any missing data using "Amelia package"
```{r}
# Check if there are any missing values:
anyNA(US_agency)
sum(is.na(US_agency))
mean(is.na(US_agency))
# the percent of missing data values is 22%

# Checking counts of missing values in columns:
colSums(is.na(US_agency))
```
 
### ploting the missing and observed data values
```{r, fig.height=7, fig.width=15}
# A visual take on the missing values might be helpful: the Amelia package has a special plotting function missmap() that will plot the dataset and highlight missing values:
library(Amelia)
missmap(US_agency, main = "Missing values vs observed")
```

### Second: **US_companies** 
    Let's calculate the number and percent of missing data and plot them
** Checking if there any missing data using "Amelia package"
```{r}
# Check if there are any missing values:
anyNA(US_companies)
sum(is.na(US_companies))
mean(is.na(US_companies))
# the percent of missing data values is approximately 20%

# Check percentages and counts of missing values in columns:
colSums(is.na(US_companies))
```

### Ploting the missing and observed data values
```{r, fig.height=7, fig.width=15}
# A visual take on the missing values might be helpful: the Amelia package has a special plotting function missmap() that will plot the dataset and highlight missing values:
library(Amelia)
missmap(US_companies, main = "Missing values vs observed")
```

**YES**, we have missing values in our datasets 
form the above analysis we can notice the variation in missing data values in each dataset

for **US_Agency** 
 1. dataset_url 
 2. subagency_abbrev 
 3. dataset_name
are the columns with highest number of missing values

for **US_Companies** 
 1. example_uses
 2. social_impact 
 3. financial_info, data_types
are the columns with highest number of missing values

**4. How is data organized in each column? Is it properly organized?**
The data is not organized at various columns of the us_agencies and us_companies. Has too many categorical values and has many non-significant columns.

**5. Is data in the proper shape for further analysis? If not, why? Explain.**
Though the data is structured, It still needs to a lot of pre-processing for any analysis. The data is not in proper shape and has a lot of missing values.

* In Us_companies, 
   + Column last_updated needs to be parsed.
   + Financial_info column has alot of unwanted tags and characters
   + data_impacts column is not organized and has unreadable characters
   + Coulmns source_count & full_time_employees has values with incorrectly feeded data and needs to be parsed
   + A lot of missing values in columns
  
* In us_agencies, 
   + used_by_fte column needs to be parsed.

**6. How will you fix this dataset? Describe the methods you will use to fix this dataset for further analysis? It can be missing values, NAs, etc. (OPTIONAL: Uploading clean dataset)**

* Any dataset with too many missing values will not give any significant/Actionable insights. 
But it depends on the importance of the data we have,
Usually a safe maximum threshold is 5% missing values, NA's of the total in each column is followed for further any analysis on the datasets. 
* We need to make sure there are no duplicates in the dataset.
```{r}
#Dataframe without Duplicates
us_ag_noDup<-US_agency %>% distinct()
us_co_noDup<-US_companies %>% distinct()
```
* If missing data for a certain feature or sample is more than 5% then we probably should leave that feature or sample out. We therefore check for features (columns) and samples (rows) where more than 5% of the data is missing using a simple function
```{r}
#Percentage of missing values at each column
col_miss_per_agency <- colMeans(is.na(US_agency))*100
col_miss_per_agency
```


```{r}
library(dplyr)
col_miss_per_company <- colMeans(is.na(US_companies))*100
col_miss_per_company
```
As we can see, there are lot of columns with missing values more than 5%

* There are alot of packages/libraries that deals with the imputation of missing values. One can also follow the conventional ways of imputation without using packages/libraries for numerical variables. 
* For mising values, five powerful R packages namely 
    + MICE
    + Amelia
    + missForest
    + Hmisc
    + mi

*source: https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/*

**7. How are the two datasets linked to each other? Is there a common "primary key" to connect the two datasets?**

As mentioned earlier, There is no Unique row identifier to access the data to perform furtherr analysis using these two datasets.  

## Exercise 2:

JSON (JavaScript Object Notation) is a most commonly used data format today and as a data scientist, you
must know how to access JSON data sets. JSON is easy for machines to parse and generate. "It is based on
a subset of the JavaScript Programming Language Standard ECMA-262 3rd Edition - December 1999.
JSON is a text format that is completely language independent [JSON.ORG]."

For this case study, you will parse JSON file, which has city traffic details. "Average Daily Traffic (ADT)"
counts are analogous to a census count of vehicles on city streets. These counts provide a close
approximation to the actual number of vehicles passing through a given location on an average weekday.
Since it is not possible to count every vehicle on every city street, sample counts are taken along larger
streets to get an estimate of traffic on half-mile or one-mile street segments. ADT counts are used by city
planners, transportation engineers, real-estate developers, marketers and many others for myriad planning
and operational purposes. Data Owner: Transportation. Time Period: 2006. Frequency: A citywide count is 
taken approximately every 10 years. A limited number of traffic counts will be taken and added to the list
periodically [https://catalog.data.gov/]"

 **1.How many variables are in the dataset?**
```{r}
# Convert JSON file to a data frame.
ChigoTraffic <- fromJSON("ChicagoTraffic.json")
# prints Total variables
print(nrow(ChigoTraffic$meta$view$columns))
```
We have a total of 23 variables in the metadata that concerns with the listed data in json file

**2. Name all the variables?**
```{r}
# Prints Column names
print(ChigoTraffic$meta$view$columns$name)
```

**3. What is the total traffic of vehicles on 100th street to 115th street?**
```{r}
# Storing Data values to a variable
CT_data<-ChigoTraffic$data
# The total traffic of vehicles on 100th street to 115th street?
for(i in 1:1279){
  if(CT_data[[i]][[11]] == "100th St"){
    print(as.numeric(CT_data[[i]][[13]]))
  }
  if(CT_data[[i]][[11]] == "101th St"){
    print(as.numeric(CT_data[[i]][[13]]))
  }
  if(CT_data[[i]][[11]] == "102th St"){
    print(as.numeric(CT_data[[i]][[13]]))
  }
  if(CT_data[[i]][[11]] == "103th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "104th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "105th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "106th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
   if(CT_data[[i]][[11]] == "107th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "108th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "109th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "110th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "111th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "112th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "113th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "114th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
  if(CT_data[[i]][[11]] == "115th St"){
    print(as.numeric(CT_data[[i]][[13]]))
    }
}
```
As we can see the traffic volume at each street in the output. The total volume of traffic between 100th Street and 115th Street is **264,000**

**4. What is the total traffic of vehicles on geolocations, (41.651861, -87.54501) and (41.66836, -87.620176)**
```{r}
for(j in 1:1279){
  if(CT_data[[j]][[15]]=="41.651861" && CT_data[[j]][[16]]== "-87.54501"){t1<-as.numeric(CT_data[[j]][[13]])}
  if(CT_data[[j]][[15]]=="41.66836" && CT_data[[j]][[16]]== "-87.620176"){t2<-as.numeric(CT_data[[j]][[13]])}
}
Total_Traffic=t1+t2
print(Total_Traffic)
```
The Total traffic on the given geolocations is 13600.